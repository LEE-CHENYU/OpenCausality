# Design registry v3 for estimator selection.
# Maps identification strategies to templates, diagnostics, and data requirements.
#
# V2 CHANGES:
# - Added credibility_weight per design (higher = more credible)
# - Designs sorted by credibility: RDD > IV > DiD > LP > Panel FE
# - Removed significance-based criteria from design selection
#
# V3 CHANGES (Generalization):
# - Added is_reaction_function flag support for designs
# - Added unit_normalization requirements
# - Added edge_type compatibility checking
# - Domain-agnostic design templates

schema_version: 3
registry_name: "default_designs"

# Global defaults applied when a design does not override.
_defaults:
  standard_errors: "cluster"  # cluster, driscoll_kraay, hac
  missing_rate_max: 0.10
  min_obs: 30

# =============================================================================
# DESIGN SPECIFICATIONS
# =============================================================================
# Each design is referenced by id in DAG edge specs.
# Sorted by credibility weight (highest first).

designs:
  # -------------------------------------------------------------------------
  # RDD: Regression Discontinuity Design
  # Credibility: 1.0 (Gold standard for local causal effects)
  # -------------------------------------------------------------------------
  - id: "RDD"
    name: "Regression Discontinuity"
    description: "Local linear RD with density and bandwidth checks."
    adapter_class: "shared.engine.adapters.rdd_adapter.RDDAdapter"
    credibility_weight: 1.0
    requirements:
      data:
        running_variable: true
        min_obs: 200
      variables:
        treatment_type: ["binary"]
        outcome_type: ["continuous", "share", "index"]
    template:
      model: "y = beta * 1[x >= c] + f(x) + e"
      standard_errors: "robust"
    diagnostics:
      required:
        - "density"           # McCrary test for manipulation
        - "bandwidth_sensitivity"  # Robust to bandwidth choice
      optional:
        - "covariate_balance" # Balance at cutoff
        - "placebo_cutoffs"   # No effect at fake cutoffs
    outputs:
      edge_card_fields: ["beta", "se", "bandwidth", "density_p"]
    notes: "Strongest design when valid; identifies LATE at cutoff only."

  # -------------------------------------------------------------------------
  # IV_2SLS: Instrumental Variables
  # Credibility: 0.9 (Strong if exclusion restriction holds)
  # -------------------------------------------------------------------------
  - id: "IV_2SLS"
    name: "Instrumental Variables 2SLS"
    description: "Two-stage least squares with explicit exclusion restriction notes."
    adapter_class: "shared.engine.adapters.iv_adapter.IV2SLSAdapter"
    credibility_weight: 0.9
    requirements:
      data:
        min_obs: 50
      variables:
        treatment_type: ["continuous"]
        outcome_type: ["continuous", "share", "index"]
        instrument_count_min: 1
    template:
      model: "x_it = pi * z_it + controls_it + u_it; y_it = beta * xhat_it + controls_it + v_it"
      standard_errors: "robust"
    diagnostics:
      required:
        - "weak_iv"           # F > 10 rule
        - "first_stage_f"     # First-stage strength
        - "exclusion_narrative"  # Must document exclusion argument
      optional:
        - "overid"            # Sargan/Hansen test if over-identified
        - "placebo_outcomes"  # Z should not predict placebo Y
    outputs:
      edge_card_fields: ["beta", "se", "first_stage_f", "overid_p"]
    notes: "Requires careful argument for exclusion restriction."

  # -------------------------------------------------------------------------
  # DID_EVENT_STUDY: Difference-in-Differences Event Study
  # Credibility: 0.8 (Good with parallel trends evidence)
  # -------------------------------------------------------------------------
  - id: "DID_EVENT_STUDY"
    name: "Difference-in-Differences Event Study"
    description: "TWFE event study with pre-trend checks."
    adapter_class: "shared.engine.adapters.did_adapter.DIDEventStudyAdapter"
    credibility_weight: 0.8
    requirements:
      data:
        panel: true
        min_periods: 12
        min_pre_periods: 4
      variables:
        treatment_type: ["binary"]
        outcome_type: ["continuous", "share", "index"]
    template:
      model: "y_it = sum_k beta_k * 1[t = k] * treated_i + alpha_i + tau_t + e_it"
      fixed_effects: ["unit", "time"]
      standard_errors: "cluster(unit)"
    diagnostics:
      required:
        - "pre_trends"        # Pre-treatment coefficients not significant
        - "placebo_event"     # No effect at fake event dates
      optional:
        - "balanced_panel"    # Composition stability
        - "never_treated"     # Clean control group
        - "staggered_robust"  # Robust to heterogeneous treatment timing
    outputs:
      edge_card_fields: ["event_time_betas", "pretrend_p", "ci_bands", "att"]
    notes: "Pre-trends are necessary but not sufficient for parallel trends."

  # -------------------------------------------------------------------------
  # LOCAL_PROJECTIONS: Jorda Local Projections
  # Credibility: 0.7 (Good for dynamic effects)
  # -------------------------------------------------------------------------
  - id: "LOCAL_PROJECTIONS"
    name: "Local Projections IRF"
    description: "Jorda local projections for dynamic impulse responses."
    adapter_class: "shared.engine.adapters.lp_adapter.LPAdapter"
    credibility_weight: 0.7
    requirements:
      data:
        time_series: true
        min_periods: 20
      variables:
        treatment_type: ["continuous"]
        outcome_type: ["continuous", "share", "index"]
    template:
      model: "y_{t+h} = beta_h * shock_t + controls_t + e_{t+h}"
      horizon_max: 12
      standard_errors: "hac"
    diagnostics:
      required:
        - "stability"         # IRF stable across specifications
        - "lag_sensitivity"   # Robust to lag structure
      optional:
        - "structural_breaks" # No breaks in sample
        - "shock_exogeneity"  # Shock is truly exogenous
    outputs:
      edge_card_fields: ["irf", "ci_bands", "horizon", "cumulative"]
    notes: "Flexible but requires well-identified shock."

  # -------------------------------------------------------------------------
  # PANEL_FE_BACKDOOR: Panel Fixed Effects
  # Credibility: 0.6 (Common but requires strong assumptions)
  # -------------------------------------------------------------------------
  - id: "PANEL_FE_BACKDOOR"
    name: "Panel FE Backdoor"
    description: "Fixed effects panel regression using a backdoor adjustment set."
    adapter_class: "shared.engine.adapters.panel_fe_adapter.PanelFEBackdoorAdapter"
    credibility_weight: 0.6
    requirements:
      data:
        panel: true
        min_periods: 8
        min_units: 10
      variables:
        treatment_type: ["continuous", "binary"]
        outcome_type: ["continuous", "share", "index"]
    template:
      model: "y_it = beta * x_it + gamma' * controls_it + alpha_i + tau_t + e_it"
      fixed_effects: ["unit", "time"]
      standard_errors: "cluster(unit)"
    diagnostics:
      required:
        - "residual_checks"   # No patterns in residuals
        - "influence"         # No outlier driving results
      optional:
        - "placebo_exposure"  # No effect on placebo outcomes
        - "pre_trends"        # Pre-treatment balance
        - "sensitivity"       # Omitted variable bias bounds
    outputs:
      edge_card_fields: ["beta", "se", "ci", "fixed_effects", "controls", "r2_within"]
    notes: "Relies on selection on observables; document backdoor path."

  # -------------------------------------------------------------------------
  # PANEL_LP_EXPOSURE_FE: Panel LP with Exposure x Shock and Fixed Effects
  # Credibility: 0.7 (Shift-share identification via cross-bank exposure variation)
  # -------------------------------------------------------------------------
  - id: "PANEL_LP_EXPOSURE_FE"
    name: "Panel LP with Exposure x Shock and Fixed Effects"
    adapter_class: "shared.engine.adapters.panel_lp_adapter.PanelLPAdapter"
    description: >
      Shift-share panel LP: differential bank response by predetermined exposure.
      Canonical model (first-differenced outcome):
        Δy_{b,t+h} = α_b + δ_t + β_h (E_b × shock_t) + Σ_ℓ ρ_{ℓ,h} Δy_{b,t-ℓ} + ε
      Time FE absorb ALL common macro events. Identification comes ONLY from (E_b × shock_t).
      Exposure E_b must be PREDETERMINED (fixed baseline average, not rolling/endogenous).
      Shock must be INNOVATION (unexpected component), not level.
    credibility_weight: 0.70
    requirements:
      data:
        panel: true
        min_periods: 8
        min_units: 3
      variables:
        treatment_type: ["interaction"]
        outcome_type: ["continuous", "share"]
        exposure_required: true
        exposure_fixed: true  # Baseline average, not rolling
        shock_type: "innovation"  # Must be AR residual or instrument, not level
    template:
      model: "Δy_{b,t+h} = α_b + δ_t + β_h (E_b × shock_t) + Σ_ℓ ρ_{ℓ,h} Δy_{b,t-ℓ} + ε"
      dependent_variable: "first_difference"  # Δy = y_{t+h} - y_{t-1}
      fixed_effects: ["bank", "time"]
      standard_errors: "driscoll_kraay"  # Or kernel HAC over time
    diagnostics:
      required:
        - "leads_test"             # Pre-trends: (E_b × shock_{t+k}) for k>0 should be insignificant
        - "leave_one_bank_out"     # β stable sign/magnitude when any bank dropped
        - "exposure_variation"     # CV of exposure >= 0.10 across banks
      optional:
        - "placebo_exposure"       # Unrelated exposure (e.g., fee_income_share) should not load
        - "regime_break"           # Pre/post stability (2015 tenge float, 2018 IFRS 9, 2020 COVID)
        - "definition_harmonization"  # KPI definitions consistent across banks
    outputs:
      edge_card_fields: ["beta_h", "se", "ci_bands", "n_units", "n_periods", "exposure_variable", "leads_test_passed"]
    interpretation:
      is: "Differential bank response per unit exposure, per unit shock"
      is_not: "Aggregate causal effect of shock on outcome; this is DIFFERENTIAL effect by exposure level"
    notes: >
      CRITICAL: If shock is common macro (same for all banks at time t) and you include time FE,
      time FE absorbs the shock level. Must identify off cross-bank heterogeneity in EXPOSURE.
      Rule: never use LEVEL of CPI or policy rate as 'shock' - use innovation/residual.

  # -------------------------------------------------------------------------
  # ACCOUNTING_BRIDGE: Mechanical accounting identity
  # Credibility: 0.9 (Deterministic by construction)
  # -------------------------------------------------------------------------
  - id: "ACCOUNTING_BRIDGE"
    name: "Accounting Bridge / Mechanical Identity"
    description: "Deterministic or near-mechanical accounting relationship."
    adapter_class: "shared.engine.adapters.accounting_bridge_adapter.AccountingBridgeAdapter"
    credibility_weight: 0.90
    requirements:
      data:
        min_obs: 1
      variables:
        treatment_type: ["continuous"]
        outcome_type: ["continuous"]
    template:
      model: "y = f(x) by accounting definition"
      standard_errors: "deterministic"
    diagnostics:
      required:
        - "identity_consistency"   # Verify the accounting relationship holds in data
      optional: []
    outputs:
      edge_card_fields: ["sensitivity", "formula", "at_values"]
    notes: "Not a causal estimate. Mechanical sensitivity at current values. Near-zero uncertainty by construction."

  # -------------------------------------------------------------------------
  # REGRESSION_KINK: Regression Kink Design
  # Credibility: 0.95 (Strong quasi-experimental design)
  # -------------------------------------------------------------------------
  - id: "REGRESSION_KINK"
    name: "Regression Kink Design"
    description: "Estimates change in slope at a known discontinuity in policy assignment."
    adapter_class: "shared.engine.adapters.regression_kink_adapter.RegressionKinkAdapter"
    credibility_weight: 0.95
    requirements:
      data:
        running_variable: true
        kink_point: true
        min_obs: 200
      variables:
        treatment_type: ["continuous"]
        outcome_type: ["continuous"]
    template:
      model: "y = a + b1*(x-c) + b2*(x-c)*1[x>c] + e"
      standard_errors: "robust"
    diagnostics:
      required:
        - "kink_specification"
        - "bandwidth_sensitivity"
      optional:
        - "density_test"
    outputs:
      edge_card_fields: ["kink_estimate", "se", "bandwidth", "density_ratio"]
    notes: "Identifies off slope change at kink; requires correctly specified kink point."

  # -------------------------------------------------------------------------
  # DOWHY_BACKDOOR: DoWhy Backdoor Adjustment
  # Credibility: 0.65 (Graph-aware, relies on correct graph specification)
  # -------------------------------------------------------------------------
  - id: "DOWHY_BACKDOOR"
    name: "DoWhy Backdoor Adjustment"
    description: "Graph-aware backdoor adjustment via DoWhy. Requires causal graph GML."
    adapter_class: "shared.engine.adapters.dowhy_backdoor_adapter.DoWhyBackdoorAdapter"
    credibility_weight: 0.65
    requirements:
      data:
        min_obs: 50
      variables:
        treatment_type: ["continuous", "binary"]
        outcome_type: ["continuous", "share", "index"]
    template:
      model: "DoWhy backdoor: E[Y | do(X)] via graph-identified adjustment set"
      standard_errors: "bootstrap"
    diagnostics:
      required:
        - "graph_validity"
        - "backdoor_identified"
      optional:
        - "refutation_random_common_cause"
        - "refutation_placebo"
    outputs:
      edge_card_fields: ["beta", "se", "backdoor_variables", "refutation_results"]
    notes: "Requires correct causal graph specification. Use refutation engine to validate."

  # -------------------------------------------------------------------------
  # DOWHY_IV: DoWhy Graph-Aware IV
  # Credibility: 0.9 (IV with graph-verified exclusion restriction)
  # -------------------------------------------------------------------------
  - id: "DOWHY_IV"
    name: "DoWhy Graph-Aware IV"
    description: "Instrumental variables with DoWhy's graph-based identification verification."
    adapter_class: "shared.engine.adapters.dowhy_iv_adapter.DoWhyIVAdapter"
    credibility_weight: 0.9
    requirements:
      data:
        min_obs: 50
      variables:
        treatment_type: ["continuous"]
        outcome_type: ["continuous", "share", "index"]
        instrument_count_min: 1
    template:
      model: "DoWhy IV: graph-verified instrument → treatment → outcome"
      standard_errors: "robust"
    diagnostics:
      required:
        - "iv_identified"
        - "graph_validity"
      optional:
        - "refutation_placebo"
    outputs:
      edge_card_fields: ["beta", "se", "instruments", "iv_identified"]
    notes: "Complements IV_2SLS by adding graph-based identification verification."

  # -------------------------------------------------------------------------
  # DOWHY_FRONTDOOR: DoWhy Frontdoor Criterion
  # Credibility: 0.85 (Novel identification not otherwise available)
  # -------------------------------------------------------------------------
  - id: "DOWHY_FRONTDOOR"
    name: "DoWhy Frontdoor Criterion"
    description: "Frontdoor criterion estimation via known mediators."
    adapter_class: "shared.engine.adapters.dowhy_frontdoor_adapter.DoWhyFrontdoorAdapter"
    credibility_weight: 0.85
    requirements:
      data:
        min_obs: 100
      variables:
        treatment_type: ["continuous", "binary"]
        outcome_type: ["continuous"]
    template:
      model: "DoWhy frontdoor: T → M → Y with unobserved T ← U → Y"
      standard_errors: "bootstrap"
    diagnostics:
      required:
        - "frontdoor_identified"
        - "mediator_validity"
      optional:
        - "refutation_placebo"
    outputs:
      edge_card_fields: ["beta", "se", "mediators"]
    notes: "Requires correctly specified mediator and no direct T→Y path."

  # -------------------------------------------------------------------------
  # DML_PLR: DoubleML Partially Linear Regression
  # Credibility: 0.75 (ML-based nuisance, cross-fitted)
  # -------------------------------------------------------------------------
  - id: "DML_PLR"
    name: "DoubleML PLR"
    description: "Partially linear regression with cross-fitted ML nuisance parameters."
    adapter_class: "shared.engine.adapters.dml_adapter.DMLAdapter"
    credibility_weight: 0.75
    requirements:
      data:
        min_obs: 100
      variables:
        treatment_type: ["continuous"]
        outcome_type: ["continuous", "share"]
    template:
      model: "Y = theta*D + g(X) + e; D = m(X) + v (cross-fitted)"
      standard_errors: "dml_cross_fitted"
    diagnostics:
      required:
        - "nuisance_scores"
      optional:
        - "sensitivity_ml_learner"
    outputs:
      edge_card_fields: ["theta", "se", "n_folds", "ml_learner"]
    notes: "Robust to functional form; requires selection-on-observables."

  # -------------------------------------------------------------------------
  # DML_IRM: DoubleML Interactive Regression Model
  # Credibility: 0.7 (Binary treatment, propensity-based)
  # -------------------------------------------------------------------------
  - id: "DML_IRM"
    name: "DoubleML IRM"
    description: "Interactive regression model for binary treatment with ML nuisance."
    adapter_class: "shared.engine.adapters.dml_adapter.DMLAdapter"
    credibility_weight: 0.7
    requirements:
      data:
        min_obs: 100
      variables:
        treatment_type: ["binary"]
        outcome_type: ["continuous", "share"]
    template:
      model: "ATE via doubly-robust scoring with cross-fitted propensity"
      standard_errors: "dml_cross_fitted"
    diagnostics:
      required:
        - "propensity_overlap"
      optional:
        - "sensitivity_ml_learner"
    outputs:
      edge_card_fields: ["ate", "se", "n_folds"]
    notes: "Binary treatment only. Doubly robust to either outcome or propensity misspecification."

  # -------------------------------------------------------------------------
  # DML_PLIV: DoubleML Partially Linear IV
  # Credibility: 0.85 (IV with ML first stage)
  # -------------------------------------------------------------------------
  - id: "DML_PLIV"
    name: "DoubleML PLIV"
    description: "Partially linear IV model with ML nuisance parameters."
    adapter_class: "shared.engine.adapters.dml_adapter.DMLAdapter"
    credibility_weight: 0.85
    requirements:
      data:
        min_obs: 100
      variables:
        treatment_type: ["continuous"]
        outcome_type: ["continuous"]
        instrument_count_min: 1
    template:
      model: "Y = theta*D + g(X) + e; D = r(X,Z) + v (IV with ML)"
      standard_errors: "dml_cross_fitted"
    diagnostics:
      required:
        - "instrument_strength"
      optional:
        - "sensitivity_ml_learner"
    outputs:
      edge_card_fields: ["theta", "se", "instruments", "n_folds"]
    notes: "Combines IV identification with ML flexibility for nuisance parameters."

  # -------------------------------------------------------------------------
  # ECONML_CATE: EconML Heterogeneous Treatment Effects
  # Credibility: 0.7 (Descriptive heterogeneity, not new identification)
  # -------------------------------------------------------------------------
  - id: "ECONML_CATE"
    name: "EconML CATE"
    description: "Conditional Average Treatment Effects via LinearDML or CausalForest."
    adapter_class: "shared.engine.adapters.econml_cate_adapter.EconMLCATEAdapter"
    credibility_weight: 0.7
    requirements:
      data:
        min_obs: 200
      variables:
        treatment_type: ["continuous", "binary"]
        outcome_type: ["continuous"]
    template:
      model: "CATE(x) = E[Y(1) - Y(0) | X=x] via DML/CausalForest"
      standard_errors: "asymptotic"
    diagnostics:
      required:
        - "heterogeneity_test"
      optional:
        - "cate_distribution"
    outputs:
      edge_card_fields: ["ate", "cate_std", "cate_p10", "cate_p90", "heterogeneity_detected"]
    notes: "Primary value is detecting treatment effect heterogeneity across subgroups."

  # -------------------------------------------------------------------------
  # CAUSALML_UPLIFT: CausalML Meta-Learners
  # Credibility: 0.5 (Descriptive, requires strong assumptions)
  # -------------------------------------------------------------------------
  - id: "CAUSALML_UPLIFT"
    name: "CausalML Uplift"
    description: "S/T/X-learner meta-learners for uplift estimation. Binary treatment."
    adapter_class: "shared.engine.adapters.causalml_adapter.CausalMLUpliftAdapter"
    credibility_weight: 0.5
    requirements:
      data:
        min_obs: 200
      variables:
        treatment_type: ["binary"]
        outcome_type: ["continuous"]
    template:
      model: "CATE via S/T/X-learner meta-learner"
      standard_errors: "bootstrap"
    diagnostics:
      required: []
      optional:
        - "uplift_calibration"
    outputs:
      edge_card_fields: ["ate", "cate_std", "learner_type"]
    notes: "Descriptive only. Not causally identified without strong unconfoundedness assumption."

# =============================================================================
# CREDIBILITY RANKING
# =============================================================================
# Summary of design credibility for reference.

credibility_ranking:
  - design: "RDD"
    weight: 1.0
    reason: "Cleanest identification at cutoff"

  - design: "IV_2SLS"
    weight: 0.9
    reason: "Strong if exclusion restriction is credible"

  - design: "DID_EVENT_STUDY"
    weight: 0.8
    reason: "Good with pre-trends evidence"

  - design: "LOCAL_PROJECTIONS"
    weight: 0.7
    reason: "Flexible dynamics, requires exogenous shock"

  - design: "PANEL_FE_BACKDOOR"
    weight: 0.6
    reason: "Common but selection-on-observables assumption"

  - design: "ACCOUNTING_BRIDGE"
    weight: 0.9
    reason: "Deterministic accounting identity, near-zero uncertainty"

  - design: "PANEL_LP_EXPOSURE_FE"
    weight: 0.7
    reason: "Shift-share identification via predetermined cross-bank exposure variation"

  - design: "REGRESSION_KINK"
    weight: 0.95
    reason: "Strong quasi-experimental design identifying off slope change"

  - design: "DOWHY_IV"
    weight: 0.9
    reason: "Graph-verified IV with DoWhy identification check"

  - design: "DOWHY_FRONTDOOR"
    weight: 0.85
    reason: "Frontdoor criterion — novel identification strategy"

  - design: "DML_PLIV"
    weight: 0.85
    reason: "IV with ML flexibility for nuisance parameters"

  - design: "DML_PLR"
    weight: 0.75
    reason: "Cross-fitted ML nuisance, robust to functional form"

  - design: "DML_IRM"
    weight: 0.7
    reason: "Doubly robust binary treatment with ML nuisance"

  - design: "ECONML_CATE"
    weight: 0.7
    reason: "Heterogeneous treatment effects via DML/CausalForest"

  - design: "DOWHY_BACKDOOR"
    weight: 0.65
    reason: "Graph-aware backdoor adjustment via DoWhy"

  - design: "CAUSALML_UPLIFT"
    weight: 0.5
    reason: "Descriptive uplift — requires strong unconfoundedness"

# =============================================================================
# EDGE TYPE RULES (NEW v3)
# =============================================================================
# Rules for handling different edge types in estimation and reporting.

edge_type_rules:
  causal:
    description: "Standard causal edge - supports shock propagation"
    propagation_allowed: true
    requires_identification: true
    report_warning: null

  reaction_function:
    description: "Endogenous policy response (e.g., Taylor rule)"
    propagation_allowed: false  # CRITICAL: Cannot use for shock propagation
    requires_identification: false  # Descriptive, not causal
    report_warning: |
      **Warning:** This edge estimates a **reaction function** (endogenous policy response),
      NOT a causal effect. It should NOT be used for shock propagation without re-specifying
      as monetary policy surprises or other exogenous variation.
    examples:
      - "cpi_to_policy_rate"      # Central bank responds to inflation
      - "fx_to_policy_rate"       # Central bank responds to depreciation
      - "output_gap_to_rate"      # Fed responds to output gap

  mechanical:
    description: "Deterministic accounting/identity relationship"
    propagation_allowed: true   # Can propagate through identities
    requires_identification: false
    report_warning: |
      **Note:** This edge is a mechanical accounting identity. The sensitivity is deterministic
      at current values and may change if underlying ratios change.
    examples:
      - "capital_to_k2"           # K2 = Capital / RWA
      - "loans_to_rwa"            # RWA = f(loans, risk_weights)

  immutable:
    description: "Validated evidence from prior research, locked from re-estimation"
    propagation_allowed: true
    requires_identification: false  # Already validated
    report_warning: null
    examples:
      - "fx_to_cpi_tradable"      # Block A validated result

# =============================================================================
# UNIT NORMALIZATION RULES (NEW v3)
# =============================================================================
# Standard units for common variable types.

unit_normalization:
  standard_units:
    percentage_point: "pp"        # 1pp = 1 percentage point
    basis_point: "bps"            # 1bps = 0.01pp
    percent: "%"                  # % change
    standard_deviation: "SD"      # 1 SD shock
    currency_billion: "bn KZT"    # Billion KZT
    ratio: "ratio"                # Unitless ratio

  # Common treatment/outcome patterns
  patterns:
    shock_to_ratio:
      treatment: "1pp shock"
      outcome: "bps ratio change"
      conversion: 100  # 1pp = 100bps

    rate_to_rate:
      treatment: "1pp rate change"
      outcome: "pp rate change"
      conversion: 1

    fx_passthrough:
      treatment: "10% depreciation"
      outcome: "pp CPI change"
      conversion: null  # No automatic conversion

  # Validation: check that chain units match
  chain_validation:
    enabled: true
    rules:
      - "If edge A outputs 'pp' and edge B expects 'pp', OK"
      - "If edge A outputs 'bps' and edge B expects 'pp', divide by 100"
      - "If units incompatible, flag ERROR and block propagation"

# =============================================================================
# DESIGN TEMPLATES LIBRARY (NEW v3)
# =============================================================================
# Pre-built templates for common causal inference designs.

design_templates:
  # DiD with staggered rollout
  staggered_did:
    base_design: "DID_EVENT_STUDY"
    extensions:
      - "callaway_santanna"   # Robust to heterogeneous treatment timing
      - "sun_abraham"         # Interaction-weighted estimator
    diagnostics_added:
      - "treatment_timing_variation"
      - "never_treated_check"

  # Synthetic control
  synthetic_control:
    description: "Synthetic control method for single-unit treatment"
    credibility_weight: 0.85
    requirements:
      data:
        panel: true
        min_pre_periods: 10
        treated_units: 1
      variables:
        treatment_type: ["binary"]
        outcome_type: ["continuous"]
    template:
      model: "y_1t = sum_j w_j * y_jt for t < T0; compare to y_1t for t >= T0"
    diagnostics:
      required:
        - "pre_treatment_fit"   # RMSPE in pre-period
        - "placebo_permutation" # Fisher permutation test
      optional:
        - "leave_one_out"       # Sensitivity to donor pool

  # Regression kink design
  regression_kink:
    description: "Regression kink design at known discontinuity in slope"
    credibility_weight: 0.95
    requirements:
      data:
        running_variable: true
        kink_point: true
        min_obs: 200
      variables:
        treatment_type: ["continuous"]
        outcome_type: ["continuous"]
    template:
      model: "y = a + b1*(x-c) + b2*(x-c)*1[x>c] + e"
    diagnostics:
      required:
        - "kink_specification"  # Is the kink point correct?
        - "bandwidth_sensitivity"

  # Bartik / shift-share
  bartik_shift_share:
    base_design: "PANEL_LP_EXPOSURE_FE"
    description: "Bartik-style shift-share IV with exogenous shares"
    credibility_weight: 0.75
    diagnostics_added:
      - "share_exogeneity"      # Are shares predetermined?
      - "shift_exogeneity"      # Are shifts exogenous?
      - "rotemberg_weights"     # Decomposition of identifying variation
