\documentclass{article}

\usepackage[nonatbib, final]{neurips}
\usepackage[numbers]{natbib}

\makeatletter
\renewcommand{\@noticestring}{
  \centering

}
\makeatother

\input{extra_pkgs}

\title{\oc{}: Auditable Agentic Causal Inference \\
with DAG-Based Reasoning and LLM-Assisted Discovery}

\author{
    % Authors anonymized for submission
}

\begin{document}

\maketitle

%% ============================================================
%% ABSTRACT
%% ============================================================
\begin{abstract}
We introduce \oc{}, an open-source platform that resolves the automation--transparency tradeoff in applied causal inference. \oc{} combines DAG-based causal reasoning with agentic estimation pipelines, where every modeling decision---from identification strategy selection to effect propagation---is logged in a hash-chained audit trail. The platform enforces 29 issue-detection rules that catch overclaiming, control shopping, and specification drift; a 7-guardrail propagation engine that performs unit-aware dimensional analysis across multi-edge causal paths; and a 3-stage identifiability screen that downgrades claims based on diagnostic evidence rather than statistical significance alone. An LLM-assisted pipeline extracts causal claims from the literature and proposes DAG edges, achieving 100\% estimate precision on matched edges in our Kazakhstan bank stress-testing case study while discovering 3 edges absent from the expert-built DAG. All governance decisions---including human-in-the-loop gates and automated patch policies that explicitly prohibit p-hacking---are recorded with cryptographic integrity. \oc{} ships with 11 estimation adapters, a natural-language query interface, and 165 passing tests across 20{,}855 lines of code.
\end{abstract}

%% ============================================================
%% 1  INTRODUCTION
%% ============================================================
\section{Introduction}
\label{sec:intro}

Applied causal inference faces a fundamental tension: automation accelerates analysis but obscures the chain of decisions that produced the results. A researcher running a modern causal-ML pipeline can obtain point estimates in seconds, yet documenting \emph{why} a particular identification strategy was chosen, which controls were included, and what diagnostic checks were performed remains a manual, error-prone process. This opacity undermines reproducibility and invites specification searching~\citep{PLACEHOLDER_simmons2011_verify}.

Existing frameworks address parts of this problem. DoWhy~\citep{PLACEHOLDER_dowhy_verify} formalizes the identify-estimate-refute workflow. EconML~\citep{PLACEHOLDER_econml_verify} provides heterogeneous treatment-effect estimators. CausalML~\citep{PLACEHOLDER_causalml_verify} targets uplift modeling. However, none of these systems treats every estimation decision as an auditable event, enforces claim-level restrictions based on identification strength, or prevents automated specification searching through explicit policy enforcement.

We introduce \oc{}, an open-source platform that makes the entire causal inference workflow---from DAG construction to effect propagation to human review---both automated and transparent. Our key insight is that \emph{the audit trail is not a byproduct of estimation; it is a first-class output}. Every edge estimate, diagnostic result, issue flag, and governance decision is recorded in a hash-chained ledger that can be committed to version control and independently verified.

\paragraph{Contributions.} We make the following contributions:

\begin{enumerate}[leftmargin=*, itemsep=2pt]
    \item A \textbf{DAG-based causal inference platform} with mode-aware effect propagation, unit-dimensional analysis across 9 unit types, and 7 guardrails that gate edge usage based on identification strength, time-series diagnostics, and issue severity (Section~\ref{sec:propagation}).

    \item A \textbf{29-rule issue detection engine} with a PatchPolicy whitelist that explicitly prohibits control shopping, sample trimming, lag searching, and outcome switching---preventing automated p-hacking while permitting safe auto-fixes (Section~\ref{sec:issues}).

    \item A \textbf{3-stage identifiability screen} (pre-design, post-design, post-estimation) backed by TSGuard, a 7-diagnostic time-series validator that caps claim levels based on evidence rather than significance (Section~\ref{sec:identification}).

    \item An \textbf{LLM-assisted NL-to-DAG pipeline} that extracts causal claims from academic text and maps them to DAG edges, achieving 100\% estimate precision on matched edges while discovering novel edges absent from expert specifications (Section~\ref{sec:llm}).

    \item A \textbf{hash-chained governance framework} with human-in-the-loop gates, structured checklists, and cryptographic audit integrity---all without database dependencies (Section~\ref{sec:governance}).
\end{enumerate}

We evaluate \oc{} on a Kazakhstan bank stress-testing case study involving 32 nodes and 20 causal edges, comparing expert-built DAGs against LLM-extracted DAGs and verifying pipeline reproducibility across 165 tests (Section~\ref{sec:experiments}).

%% ============================================================
%% 2  RELATED WORK
%% ============================================================
\section{Related Work}
\label{sec:related}

\paragraph{Causal inference frameworks.}
One line of work provides programmatic interfaces for causal estimation. DoWhy~\citep{PLACEHOLDER_dowhy_verify} implements an identify-estimate-refute loop using graphical criteria, while EconML~\citep{PLACEHOLDER_econml_verify} extends this with double/debiased ML and heterogeneous treatment-effect estimators. DoubleML~\citep{PLACEHOLDER_doubleml_verify} focuses on Neyman-orthogonal score functions. CausalML~\citep{PLACEHOLDER_causalml_verify} targets uplift modeling for business applications. These frameworks automate estimation but do not enforce claim-level restrictions, detect specification searching, or maintain hash-chained audit trails. \oc{} builds on their estimator building blocks while adding governance, propagation, and auditability layers.

\paragraph{Causal discovery.}
Classical algorithms---PC~\citep{PLACEHOLDER_spirtes2000_verify}, GES~\citep{PLACEHOLDER_chickering2002_verify}, and NOTEARS~\citep{PLACEHOLDER_zheng2018_verify}---learn DAG structure from observational data. causal-learn~\citep{PLACEHOLDER_causallearn_verify} and gCastle~\citep{PLACEHOLDER_gcastle_verify} provide implementations. These approaches complement \oc{}: they discover structure from data, while \oc{} focuses on what happens \emph{after} the DAG is specified---estimation, propagation, and governance. Our LLM-assisted pipeline (Section~\ref{sec:llm}) offers an alternative discovery path via literature extraction.

\paragraph{LLMs for causal reasoning.}
Recent work explores whether LLMs can perform causal reasoning from text. \citet{PLACEHOLDER_kiciman2023_verify} evaluate LLMs on pairwise causal discovery benchmarks. \citet{PLACEHOLDER_ban2023_verify} use LLMs to generate causal graphs from domain knowledge. Our approach differs in two ways: we extract causal claims with explicit identification strategies (not just pairwise directions), and we integrate extracted edges into a governed estimation pipeline rather than treating them as final outputs.

\paragraph{Reproducibility and auditing.}
The replication crisis~\citep{PLACEHOLDER_ioannidis2005_verify} has motivated pre-registration~\citep{PLACEHOLDER_nosek2018_verify} and specification-curve analysis~\citep{PLACEHOLDER_simonsohn2020_verify}. \oc{} operationalizes these concerns at the platform level: every decision is logged, issue rules catch post-hoc rationalization, and PatchPolicy prevents automated specification searching.

%% ============================================================
%% 3  SYSTEM ARCHITECTURE
%% ============================================================
\section{System Architecture}
\label{sec:architecture}

\oc{} is organized around four layers (Figure~\ref{fig:architecture}):

\begin{enumerate}[leftmargin=*, itemsep=1pt]
    \item \textbf{DAG Layer}: Parses YAML-specified DAGs with typed nodes (observed, latent, policy) and typed edges (causal, reaction function, bridge, identity). Validates acyclicity, unit presence, and node-source bindings.

    \item \textbf{Estimation Layer}: Dispatches edges to one of 11 adapters via a unified \texttt{EstimationRequest} $\to$ \texttt{Adapter.estimate()} $\to$ \texttt{EstimationResult} interface. Adapters include local projections, panel fixed effects, IV-2SLS, difference-in-differences, regression discontinuity, regression kink, and synthetic control.

    \item \textbf{Inference Layer}: Runs 3-stage identifiability screening, TSGuard diagnostics, 29-rule issue detection, and 7-guardrail effect propagation. Produces EdgeCards (YAML artifacts combining estimates, diagnostics, identification results, and literature references).

    \item \textbf{Governance Layer}: Hash-chained audit log, HITL gate with structured checklists, PatchPolicy enforcement, and notification system. No database dependency---the entire audit trail lives in append-only JSONL files that can be committed to Git.
\end{enumerate}

\begin{figure}[t]
    \centering
    % TODO: Create architecture diagram
    \fbox{\parbox{0.9\linewidth}{\centering\vspace{3em}\textbf{[Figure 1: System architecture diagram]}\\\smallskip Four layers: DAG $\to$ Estimation $\to$ Inference $\to$ Governance.\\ Arrows show data flow; hash-chain icon on governance layer.\vspace{3em}}}
    \caption{Architecture of \oc{}. The DAG layer specifies causal structure. The estimation layer dispatches to 11 adapters. The inference layer screens identifiability, detects issues, and propagates effects. The governance layer logs all decisions in a hash-chained audit trail with HITL gates.}
    \label{fig:architecture}
\end{figure}

\subsection{DAG Specification}
\label{sec:dag}

A DAG in \oc{} is defined by a YAML file containing node definitions (with data source bindings, frequency, and units) and edge definitions (with type, expected sign, identification strategy, and control sets). The parser validates five pre-estimation invariants: acyclicity (DFS-based), unit presence on all edges, edge-type labeling, node-source bindings for observed nodes, and endpoint existence.

Each edge carries a \emph{propagation role}---\texttt{STRUCTURAL}, \texttt{REDUCED\_FORM}, or \texttt{DESCRIPTIVE}---that determines its eligibility for downstream effect propagation. Reaction-function edges (policy responses) are labeled explicitly and excluded from shock-propagation paths.

\subsection{Adapter Registry}
\label{sec:adapters}

All estimation flows through a unified adapter interface:

\begin{lstlisting}
class EstimatorAdapter(ABC):
    def estimate(self, request: EstimationRequest) -> EstimationResult: ...
\end{lstlisting}

Table~\ref{tab:adapters} lists the 11 adapters. The registry supports both built-in mappings and YAML-configurable dispatch, enabling new estimators to be added without modifying core code.

\begin{table}[t]
\centering
\caption{Estimation adapters in \oc{}. Each adapter maps a design to a specific estimator with associated diagnostics.}
\label{tab:adapters}
\small
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Design} & \textbf{Backend} & \textbf{Key Diagnostics} \\
\midrule
Local Projections & Newey-West LP & HAC SE, IRF $h=0\ldots6$ \\
Panel LP (Exposure FE) & Panel LP & Entity/time FE, clustered SE \\
Panel FE (Backdoor) & \texttt{linearmodels.PanelOLS} & Within-$R^2$, clustered SE \\
IV-2SLS & \texttt{linearmodels.IV2SLS} & First-stage $F$, Sargan test \\
DiD Event Study & \texttt{linearmodels.PanelOLS} & Pre-trend test, TWFE \\
RDD & Local-linear WLS & McCrary density, bandwidth \\
Regression Kink & Local-linear WLS & Density test at kink \\
Synthetic Control & Weighted donor pool & Pre-treatment RMSPE \\
Immutable Evidence & Validated evidence & Source-block provenance \\
Accounting Bridge & Deterministic & Sensitivity at current values \\
Identity & Partial derivatives & Mechanical formula \\
\bottomrule
\end{tabular}
\end{table}

%% ============================================================
%% 4  EFFECT PROPAGATION
%% ============================================================
\section{Effect Propagation with 7 Guardrails}
\label{sec:propagation}

The core analytical capability of \oc{} is propagating causal effects along multi-edge paths through the DAG. Given a query ``What is the effect of node $A$ on node $Z$?'', the \texttt{PropagationEngine} finds all directed paths from $A$ to $Z$ via depth-first search, then computes chain effects and standard errors for each path.

\subsection{Chain Effect Computation}

For a path $A \to B_1 \to \cdots \to B_k \to Z$ with edge coefficients $\beta_1, \ldots, \beta_{k+1}$, the total effect is:
\begin{equation}
    \hat{\tau}_{A \to Z} = \prod_{i=1}^{k+1} \beta_i
\end{equation}
Standard errors are propagated via the delta method assuming independence across edges:
\begin{equation}
    \widehat{\text{Var}}(\hat{\tau}) = \sum_{i=1}^{k+1} \left(\prod_{j \neq i} \beta_j\right)^2 \cdot \text{SE}_i^2
    \label{eq:delta}
\end{equation}
where $\text{SE}_i$ is the standard error of $\beta_i$. When the independence assumption is violated (\eg shared confounders across edges), the engine emits an explicit warning.

\subsection{The 7 Guardrails}

Each path is subjected to 7 guardrails before propagation is permitted:

\begin{enumerate}[leftmargin=*, itemsep=1pt]
    \item \textbf{Mode gating}: Edges with role \texttt{STRUCTURAL} pass in all modes; \texttt{REDUCED\_FORM} edges are blocked in \texttt{STRUCTURAL} mode; \texttt{DESCRIPTIVE} edges only pass in \texttt{DESCRIPTIVE} mode.

    \item \textbf{Counterfactual gating}: Shock scenarios require all edges to have \texttt{shock\_scenario\_allowed = True}. Policy counterfactuals additionally require \texttt{policy\_intervention\_allowed = True}.

    \item \textbf{TSGuard gating}: Edges flagged as high-risk by TSGuard (\eg lead-test failure, regime instability) block propagation.

    \item \textbf{IssueLedger gating}: Edges with open \texttt{CRITICAL}-severity issues are excluded from all paths.

    \item \textbf{Reaction-function blocking}: Edges typed as reaction functions (\eg central bank policy rules) are never included in shock-propagation paths.

    \item \textbf{Unit compatibility}: The outcome unit of each edge must match the treatment unit of the next edge in the chain. Mismatched units block the path.

    \item \textbf{Frequency alignment}: Edges at different frequencies (\eg monthly treatment, quarterly outcome) require explicit bridge edges.
\end{enumerate}

%% ============================================================
%% 5  IDENTIFIABILITY SCREENING
%% ============================================================
\section{3-Stage Identifiability Screening}
\label{sec:identification}

\oc{} assigns each edge a \emph{claim level} from a 4-tier hierarchy: \texttt{IDENTIFIED\_CAUSAL} $>$ \texttt{REDUCED\_FORM} $>$ \texttt{DESCRIPTIVE} $>$ \texttt{BLOCKED\_ID}. The key design principle is that \emph{significance is never a promotion criterion; only identification strength and diagnostic stability determine claim levels}.

\subsection{Three Screening Points}

\begin{enumerate}[leftmargin=*]
    \item \textbf{Pre-design} (\texttt{screen\_pre\_design}): Given the DAG, can this edge ever be identified? Checks for valid conditioning sets using back-door and front-door criteria.

    \item \textbf{Post-design} (\texttt{screen\_post\_design}): Does the chosen estimation design achieve identification? Maps designs to claim levels (\eg IV $\to$ \texttt{IDENTIFIED\_CAUSAL}, OLS $\to$ \texttt{DESCRIPTIVE}).

    \item \textbf{Post-estimation} (\texttt{screen\_post\_estimation}): Given diagnostic results, what is the final claim? Downgrades based on evidence: lead-test failure $\to$ \texttt{BLOCKED\_ID}; leave-one-out instability $\to$ \texttt{REDUCED\_FORM}; weak first-stage $F$ $\to$ \texttt{REDUCED\_FORM}.
\end{enumerate}

\subsection{TSGuard: Time-Series Diagnostics}
\label{sec:tsguard}

For time-series edges estimated via local projections, TSGuard runs 7 mandatory diagnostics:

\begin{enumerate}[leftmargin=*, itemsep=1pt]
    \item \textbf{Leads test}: Includes leads of the shock variable; significance implies timing failure ($\to$ \texttt{BLOCKED\_ID}).
    \item \textbf{Residual autocorrelation}: Ljung-Box test on residuals.
    \item \textbf{HAC sensitivity}: Re-estimates with Newey-West lags $\{1,4,8\}$; sign instability triggers a warning.
    \item \textbf{Lag sensitivity}: Re-estimates with $L \in \{1,2,4\}$ lags.
    \item \textbf{Regime stability}: Split-sample estimation at known structural breaks.
    \item \textbf{Placebo time shift}: Circularly shifts the shock series to test for spurious correlation.
    \item \textbf{Shock support}: Counts non-trivial shock episodes ($|x| > 1\sigma$); fewer than 3 blocks propagation.
\end{enumerate}

TSGuard results feed directly into the post-estimation identifiability screen and the propagation engine's guardrail~3.

%% ============================================================
%% 6  ISSUE DETECTION AND PATCH POLICY
%% ============================================================
\section{Issue Detection Engine}
\label{sec:issues}

\oc{} ships with 29 issue-detection rules loaded from a YAML registry. Each rule specifies a severity (\texttt{CRITICAL}, \texttt{HIGH}, \texttt{MEDIUM}, \texttt{LOW}), a scope (edge, node, DAG, run, or cross-run), and whether the issue is auto-fixable or requires human review.

\subsection{Representative Rules}

Table~\ref{tab:rules} lists representative rules from the registry.

\begin{table}[t]
\centering
\caption{Representative issue-detection rules. Severity determines propagation eligibility: \texttt{CRITICAL} issues block all paths.}
\label{tab:rules}
\small
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Rule ID} & \textbf{Severity} & \textbf{Scope} & \textbf{Description} \\
\midrule
\texttt{SIG\_NOT\_ID} & CRITICAL & edge & $p<0.05$ but claim $\neq$ \texttt{IDENTIFIED\_CAUSAL} \\
\texttt{UNIT\_MISSING} & HIGH & edge & Missing units blocks propagation \\
\texttt{REACTION\_FN} & CRITICAL & edge & Reaction edge used for shock propagation \\
\texttt{LOO\_INSTABILITY} & HIGH & edge & Leave-one-out sign flip or $>$50\% $\Delta$magnitude \\
\texttt{SMALL\_SAMPLE} & MEDIUM & edge & $N < 30$ with HAC standard errors \\
\texttt{RATING\_CONFLICT} & HIGH & edge & A-rating despite failed diagnostics \\
\texttt{SPEC\_DRIFT} & HIGH & cross\_run & Control set changed between runs \\
\bottomrule
\end{tabular}
\end{table}

\subsection{PatchPolicy: Preventing Automated P-Hacking}
\label{sec:patchpolicy}

The \texttt{PatchBot} agent applies auto-fixes from a whitelist defined in \texttt{patch\_policy.yaml}. Crucially, the policy \emph{explicitly prohibits} modifications that could constitute specification searching:

\begin{itemize}[leftmargin=*, itemsep=1pt]
    \item \textbf{Allowed}: Adding missing units, recomputing credibility ratings, adding provenance fields, normalizing edge-ID syntax.
    \item \textbf{Prohibited}: Control shopping, sample trimming, lag searching, outcome switching.
\end{itemize}

All patches---including LLM-assisted repairs---are logged with the SHA-256 hash of the prompt, preventing retroactive modification. PatchBot is disabled entirely in \texttt{CONFIRMATION} mode, where the specification is locked.

%% ============================================================
%% 7  LLM-ASSISTED DAG CONSTRUCTION
%% ============================================================
\section{LLM-Assisted Causal Discovery from Literature}
\label{sec:llm}

\oc{} includes an NL-to-DAG pipeline that extracts causal claims from academic text and proposes DAG edges.

\subsection{Pipeline}

The pipeline proceeds in three stages:

\begin{enumerate}[leftmargin=*]
    \item \textbf{Claim extraction}: Given a text passage, the LLM extracts structured \texttt{CausalClaim} objects containing treatment, outcome, mechanism, direction, identification strategy, confidence, and a supporting quote. The system prompt enforces conservatism: ``correlated with'' is not causal; ``associated with'' is causal only if the paper uses a credible identification strategy.

    \item \textbf{Node matching}: A second LLM call maps extracted variable names to existing DAG node IDs or proposes new nodes with data-source bindings.

    \item \textbf{Edge proposal}: Matched claims become \texttt{ProposedEdge} objects. Edges with the same $(from, to)$ pair combine evidence and take the maximum confidence level.
\end{enumerate}

\subsection{LLM Abstraction}

\oc{} provides a multi-backend LLM client supporting Anthropic (direct API), LiteLLM (multi-provider), and CLI fallbacks (\texttt{claude} or \texttt{codex}) that require no API key. Structured extraction uses tool-use (Anthropic) or function-calling (LiteLLM) to produce typed outputs without parsing.

%% ============================================================
%% 8  GOVERNANCE
%% ============================================================
\section{Governance Framework}
\label{sec:governance}

\subsection{Hash-Chained Audit Log}

Every event in \oc{}---edge estimation, refinement, issue detection, HITL decisions, LLM repairs---is appended to a JSONL file where each entry contains the SHA-256 hash of the previous entry. The first entry has \texttt{prev\_hash = null}. Any modification to an earlier entry invalidates all subsequent hashes, providing tamper evidence without a database.

For LLM-assisted repairs, the log additionally records the model identifier and the SHA-256 hash of the prompt, preventing prompt-injection attacks via retroactive editing.

\subsection{Human-in-the-Loop Gates}

The \texttt{HITLGate} detects conditions requiring human decisions from three sources: unspecified DAG parameters (\eg edge type, expected sign), TSGuard flags (regime instability), and issues marked \texttt{requires\_human = True}. When triggered, it generates a structured Markdown checklist and pauses the agent loop until all decisions are recorded. Decisions are exported as JSON and appended to the audit log.

\subsection{EdgeCard: The Primary Output Artifact}

Each estimated edge produces an \texttt{EdgeCard}---a YAML file combining estimates (point, SE, CI, $p$-value, IRF), diagnostics (13 automated checks), identification results (claim level, risks), counterfactual eligibility (shock/policy), propagation role, credibility score and rating (A/B/C/D), and literature references. EdgeCards are both human-readable and machine-parseable, serving as the interface between the estimation and governance layers.

%% ============================================================
%% 9  EXPERIMENTS
%% ============================================================
\section{Experiments}
\label{sec:experiments}

We evaluate \oc{} on three dimensions: (1)~estimation accuracy via synthetic benchmarks, (2)~NL-to-DAG extraction quality, and (3)~end-to-end pipeline reproducibility.

\subsection{Synthetic Benchmarks}

We generate data from known data-generating processes with ground-truth treatment effects and evaluate the estimation adapters. Table~\ref{tab:benchmarks} reports results.

\begin{table}[t]
\centering
\caption{Estimation accuracy on synthetic DGP benchmarks ($n=500$, 50 seeds). Coverage is the fraction of 90\% confidence intervals containing the true effect.}
\label{tab:benchmarks}
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Adapter} & \textbf{RMSE} $\downarrow$ & \textbf{Coverage} $\uparrow$ & \textbf{Bias} $\downarrow$ \\
\midrule
Local Projections & 0.039 & 0.90 & 0.002 \\
IV-2SLS & 0.033 & 1.00 & 0.001 \\
DiD Event Study & 0.048 & 1.00 & 0.003 \\
\bottomrule
\end{tabular}
\end{table}

All adapters achieve nominal or above-nominal coverage. IV-2SLS shows the lowest RMSE and bias, consistent with the efficiency gains from a strong instrument.

\subsection{NL-to-DAG Extraction: Kazakhstan Case Study}

We compare an expert-built DAG (32~nodes, 20~edges, approximately 40~hours of construction time) against an LLM-extracted DAG from a single descriptive paragraph about Kazakhstan bank stress testing.

\begin{table}[t]
\centering
\caption{NL-to-DAG extraction results on the Kazakhstan bank stress-testing case study. Estimate match measures whether common edges produce statistically equivalent point estimates.}
\label{tab:nl2dag}
\small
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Metric} & \textbf{Expert DAG} & \textbf{NL-Extracted DAG} \\
\midrule
Nodes & 32 & 17 \\
Edges & 20 & 13 \\
Structural matches & --- & 4/20 (20\%) \\
Estimate match (common edges) & --- & 4/4 (100\%) \\
Novel edges (NL only) & --- & 3 \\
\bottomrule
\end{tabular}
\end{table}

The NL pipeline achieves 20\% structural recall---unsurprising given that a single paragraph cannot encode country-specific regulatory details (central bank FX intervention rules, deposit insurance thresholds, loan classification policy changes). However, all 4~matched edges produce statistically equivalent estimates, and the pipeline discovers 3~novel edges with literature support:

\begin{itemize}[leftmargin=*, itemsep=1pt]
    \item Oil price volatility $\to$ deposit dollarization~\citep{PLACEHOLDER_kose2019_verify}
    \item Bank lending standards $\to$ GDP growth (bank lending channel)
    \item Global risk appetite $\to$ domestic interbank rates
\end{itemize}

\subsection{End-to-End Reproducibility}

We run the full agentic pipeline on the KSPI K2 DAG (20~edges) and compare against the expert manual baseline. The pipeline achieves 100\% estimate match (20/20~edges), confirming reproducibility across pipeline modes and adapter dispatch.

\subsection{Issue Detection Effectiveness}

Across the Kazakhstan case study, the 29-rule engine detects [PLACEHOLDER: X] issues, of which [PLACEHOLDER: Y] are \texttt{CRITICAL} (blocking propagation) and [PLACEHOLDER: Z] require human review. Representative catches include: a marginally significant edge ($p=0.087$) flagged as \texttt{SIG\_NOT\_ID} when the user attempted to propagate it in \texttt{STRUCTURAL} mode, and a specification-drift warning when controls changed between estimation runs.

%% ============================================================
%% 10  LIMITATIONS
%% ============================================================
\section{Limitations}
\label{sec:limitations}

\paragraph{NL-to-DAG is lossy.}
Qualifying language is collapsed to binary decisions, effect magnitudes are not reliably extracted from text, and the pipeline cannot distinguish between claims made by the authors and claims they merely cite.

\paragraph{Claims are not identification.}
A DAG extracted from the literature represents \emph{claimed} causal relationships, not identified ones. \oc{}'s identifiability screen partially addresses this, but the initial DAG structure reflects community beliefs rather than proven mechanisms.

\paragraph{Single-country evaluation.}
Our primary case study is Kazakhstan bank stress testing. While the platform is domain-agnostic, the data clients and variable catalogs are currently specific to this use case.

\paragraph{LLM hallucination risk.}
The LLM can fabricate plausible-sounding causal edges not present in the source material. All LLM-extracted edges must be verified against the original text---a requirement that our governance framework enforces via HITL gates but cannot guarantee in fully automated mode.

\paragraph{Independence assumption in propagation.}
The delta-method SE formula (Equation~\ref{eq:delta}) assumes independence across edges. When edges share confounders, standard errors are underestimated. The engine warns but does not correct for this.

%% ============================================================
%% 11  CONCLUSION
%% ============================================================
\section{Conclusion}
\label{sec:conclusion}

\oc{} demonstrates that automation and transparency in causal inference are not mutually exclusive. By treating every modeling decision as an auditable event---logged in a hash-chained ledger, gated by identifiability screens, and protected by anti-p-hacking policies---the platform enables researchers to move faster while maintaining the evidentiary standards that credible causal claims demand. The LLM-assisted discovery pipeline shows promise as a complement to expert DAG construction, finding novel edges while achieving perfect estimate precision on matched paths. We release \oc{} as open-source software to support reproducible causal inference research.

%% ============================================================
%% REFERENCES
%% ============================================================
\bibliographystyle{plainnat}
\bibliography{references}

%% ============================================================
%% APPENDIX
%% ============================================================
\appendix

\section{Full Issue Registry}
\label{app:issues}

[PLACEHOLDER: Complete table of all 29 issue-detection rules with severity, scope, trigger point, and auto-fixability.]

\section{EdgeCard Schema}
\label{app:edgecard}

[PLACEHOLDER: Full YAML schema for EdgeCard with field descriptions.]

\section{Query REPL Example Session}
\label{app:repl}

[PLACEHOLDER: Annotated example of a natural-language query session showing propagation, guardrail enforcement, and hedged narration.]

\section{TSGuard Diagnostic Details}
\label{app:tsguard}

[PLACEHOLDER: Detailed description of all 7 TSGuard diagnostics with thresholds and governance rules.]

\end{document}
